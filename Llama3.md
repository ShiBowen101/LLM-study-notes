MOE（Mixture of Experts，专家混合模型）是一种神经网络架构，旨在通过选择性激活部分子网络来提高模型的计算效率和性能。MOE的基本思想是通过“专家网络”（experts）来分配任务，并由一个“门控机制”（gating mechanism）来选择最合适的专家进行特定任务的处理。
## MOE架构补充
### MOE架构的核心组成：

1. **专家网络（Experts）**：
    
    - 专家网络是一组不同的子神经网络，每个网络可以专注于不同的任务或数据模式。这些子网络独立进行计算，通常是标准的前馈神经网络或变体，如卷积神经网络（CNN）或递归神经网络（RNN）。
2. **门控网络（Gating Network）**：
    
    - 门控网络是一个小型网络，负责在输入数据到来时决定激活哪些专家网络。它为每个专家分配一个权重（或概率），通过这些权重，模型可以选择最适合当前输入的专家网络。
    - 通常，门控网络会基于输入数据对每个专家分配的权重进行软选择（Softmax），激活最相关的专家。
3. **稀疏激活**：
    
    - MOE的一个关键特点是稀疏激活机制。即每个输入仅激活部分专家（例如，两个或三个），而不是激活所有专家。通过这种方式，MOE架构大大减少了计算开销，因为只有少量子网络需要运行，而非整个模型。

### MOE的工作流程：

1. 输入数据被传递到**门控网络**。
2. 门控网络根据输入生成一个向量，表示每个专家的权重。
3. 基于这些权重，选择少量**专家网络**来处理该输入（通常是稀疏激活，只有少部分专家参与计算）。
4. 被选中的专家网络分别对输入进行处理，并输出结果。
5. 最终的输出是这些专家网络结果的加权组合，权重由门控网络的输出决定。
- Llama3不是MOE架构
## COT补充：
### COT 的工作原理：

1. **逐步推理**：COT通过逐步引导模型思考，避免一次性输出最终答案。这种分解复杂问题的策略帮助模型更清楚地理解问题，并通过多个推理步骤逐步接近正确答案。
    
2. **显式步骤展示**：在生成过程中，模型不仅仅是直接给出结果，还会显式展示出推理的中间步骤。例如，解决数学问题时，模型会首先写出推导过程，然后给出最终答案。
    
3. **模型提示**：在训练或测试过程中，使用带有提示的示例来鼓励模型逐步思考。例如，可以通过提示模型回答时展示推理步骤，让它模仿人类逐步解决问题的方式。
### 具体示例：

假设我们让模型解答一个数学问题：  
**问题**：12 个苹果和 8 个香蕉，每人 4 个水果，能分给多少人？

使用 COT 策略，模型的回答步骤可能是：

1. 总共有 12 + 8 = 20 个水果。
2. 每人分 4 个水果。
3. 用总的水果数量除以每人分得的水果数量：20 ÷ 4 = 5。
4. 最终答案：能分给 5 个人。

这种逐步的推理过程就是 COT 的典型表现，而不是直接输出“5个人”。

- 所以通过COT获得高准确率实际上并没有直接zero-shot的高准确率具有说服力。

## Minhash补充：
**Minhash** 是一种用于快速估计两个集合之间相似度的哈希技术，主要用于**近似集合相似性计算**，特别是计算**Jaccard相似系数**。Minhash 常被应用于大规模数据集的去重、文档查重、以及集合相似性搜索等问题中。

### Jaccard相似系数

在理解 Minhash 之前，首先要了解 Jaccard 相似系数。Jaccard 相似系数是衡量两个集合相似度的指标，定义为两个集合交集的大小除以它们并集的大小：
$$Jaccard(A, B) = (\frac{|A \cap B|}{|A \cup B|})$$


这个度量在集合相似度比较时非常常用。然而，当集合非常大时，直接计算 Jaccard 系数的成本较高，这时就可以使用 Minhash 来快速近似估计。

### Minhash 的核心思想

Minhash 的核心思想是通过对集合应用多个哈希函数来生成签名（signature），以此来近似估计集合之间的 Jaccard 相似度。

#### Minhash 工作流程：

1. **哈希函数的构造**：假设我们有一组哈希函数 h1,h2,...,hnh1​,h2​,...,hn​，这些哈希函数会将集合中的每个元素映射为不同的整数值。
    
2. **最小哈希值的计算**：对于每个哈希函数 hihi​，我们对集合 AA 和 BB 中的所有元素分别应用哈希函数，选择输出的最小值作为该集合在这个哈希函数下的最小哈希值。
    
    比如，对于集合 AA 和哈希函数 h1h1​，我们计算集合中每个元素的哈希值，并选出最小的哈希值：
    
    $$\text{minhashA}(h_1) = \min(h_1(a_1), h_1(a_2), \dots, h_1(a_n))$$
3. **签名矩阵**：通过对多个哈希函数重复上述操作，我们为每个集合生成一个**签名**，该签名可以看作是集合的简化表示。每个集合都会对应一个签名矩阵中的一行。Minhash的长度取决于hash函数的设定数量。
    
4. **相似度估计**：通过比较两个集合签名的相似性，计算它们在多个哈希函数下的最小哈希值是否相同。如果签名的很多哈希值相同，那么可以认为集合的 Jaccard 相似度较高。**签名相同的概率**正好是两个集合的 Jaccard 相似度的近似值。
# 预训练
- 1.数据清洗：包括网页抓取，信息提取过滤等等（针对Llama3它刻意的去除了markdown的标识符，但是markdown的优势可以通过代码体现出文字的空间信息）
- 通过Minhash作为相似度的度量来去除相似度高的文章
- 通过对于数据的质量以及分类来调整各项数据的权重形成混合的数据
- ![image.png](https://raw.githubusercontent.com/ShiBowen101/PicGo_imgs/main/obsidian/20240908010713.png)
- 退火数据：将较为简单或者低质的数据用于预训练后，再混合高质量数据调整学习率后再次训练，可跳出局部最优点。
- ![image.png](https://raw.githubusercontent.com/ShiBowen101/PicGo_imgs/main/obsidian/20240908011458.png)
# 模型架构
- 采用GQA即：在多头注意力机制中将多个按照一定的比率分组，同一个组中$W_q$,$W_k$都相同，形成相同的query和key矩阵，可以减少计算的时间以及缓存
- 在进行self-attention时，来自不同文档的句子彼此不进行融合。
- ![image.png](https://raw.githubusercontent.com/ShiBowen101/PicGo_imgs/main/obsidian/20240908140212.png)
- token变长：可以提升上下文窗口的长度
- scalling law补充：Scaling laws 是深度学习领域中用于描述**模型性能**与其**规模**（如参数数量、数据量、计算资源等）之间关系的经验法则。通过这些规律，我们可以理解如何有效地扩大模型规模来获得更好的性能，以及扩展的代价。
- ![image.png](https://raw.githubusercontent.com/ShiBowen101/PicGo_imgs/main/obsidian/20240908143247.png)
- Llama3利用小参数模型以及Llama2的针对参数指标，以及不同类别的数据通过Scalling Law（通过计算预测下一个token的损失）进行预测，预测出最优的Llama3的配置。
- ![image.png](https://raw.githubusercontent.com/ShiBowen101/PicGo_imgs/main/obsidian/20240908143629.png)
- tesnsor parallelism：将所有层裂开
- pipeline parallelism：将每个层分给不同的GPU
- data parallelism：将数据切开，分到不同组的GPU
- context parallelism：将序列分开（？）
- ![image.png](https://raw.githubusercontent.com/ShiBowen101/PicGo_imgs/main/obsidian/20240908173239.png)
# 训练
- 逐步提升训练的序列长度，直到最终的目标128K
- 最后在128K时对模型进行退火处理，过程中将学习率线性降为0，并将这一过程的模型做平均作为最终的模型
- ![image.png](https://raw.githubusercontent.com/ShiBowen101/PicGo_imgs/main/obsidian/20240909224122.png)
- 待续
